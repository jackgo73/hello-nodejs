---
title: Statistics03-KNN
date: 2017-10-27 08:20:25
categories: Statistics
tags: 
    - ml
---

# K近邻

k近邻法是一种基本分类与回归方法，这本书只讨论分类问题中的k近邻法。

- 输入：实例的特征向量
- 输出：实例的类别，可以取多类

要点：

- k近邻法假设给定一个训练数据集，其中实例类别已定。没有显示的学习过程。
- 分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。
- k近邻法利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。

三要素：

- 选择
- 距离度量
- 分类决策规则

k >>> 在训练集中找出与x最近邻的k个点，涵盖这k个点的x的邻域记作$$N_{k}$$

- 优点 ：精度高、对异常值不敏感、无数据输入假定。
- 缺点：计算复杂度高、空间复杂度高。
- 适用数据范围：数值型和标称型。 

## K近邻的工程化流程

(1)收集数据：可以使用任何方法。
(2)准备数据：距离计算所需要的数值，最好是结构化的数据格式。
(3)分析数据：可以使用任何方法。
(4)训练算法：此步驟不适用于1 近邻算法。
(5)测试算法：计算错误率。
(6)使用算法：首先需要输入样本数据和结构化的输出结果，然后运行女-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。 

##k近邻模型

### 模型

单元：距离该点比其他点更近的所有点组成一个区域。

![](/images/2017-10-27-Statistics03-KNN-0.jpg)

### 距离度量

- 特征空间中两个实例点的举例是其相似程度的反应
- k近邻模型的特征空间一般是n维实数向量空间$$R^{N}$$，使用的是欧氏距离。

![](/images/2017-10-27-Statistics03-KNN-1.jpg)

### k值得选择

k的选择对结果产生很大影响！

- 小k值：整体模型变得复杂，容易发生过拟合。
- 大k值：整体模型变得简单。
- k=N：预测他属于在训练实例中最多的类。模型过于简单，忽略大量有用信息。

应用中，通常区一个较小的值，通常采用交叉验证法选取最优k值。

### 分类决策规则

多数表决规则的推导：

![](/images/2017-10-27-Statistics03-KNN-2.jpg)

最后的公式的含义：
$$
\sum_{x_{i}\subseteq N_{k}(x)}I(y_{i}=c_{j})
$$
最近的一些点组成$$N_{k}$$，举个例子：举例最近的有五个点，这个求和会遍历这五个点然后求和，那么和cj相同次数比较多的那些(x,y)点会使这个求和公式整理较大，记住这个cj是常数， xy是变量点。所以合适的cj会使整体较大，使经验风险最小。



经验风险：模型关于训练数据集的平均损失。

## kd树

实现是最重要的问题就是实现快速k近邻搜索。

最简单使用线性扫描：计算输入实例与每一个训练实例的举例。不可行。

### 构造kd树

- 概念：对k维空间中的实例点进行存储，以便对其进行快速检索的树形数据结构。
- kd树是二叉树。表示对k维空间的一个划分。
- 构造kd树相当于不断的用垂直于坐标轴的超平面将k维空间切分。构成一系列k维超矩形区域。
- kd树的每一个节点对应于一个k为超矩形区域。



方法：

- 根节点对应于k维空间中包含所有实例点的超矩形区域。
- 在超矩形区域（根节点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个**垂直于选定坐标轴！**的超平面，实例被分到两个子区域（子节点）同时将实例保存在相应的节点上。直到区域内没有实例时终止。
- 通常选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数为切分点，这样的kd树是平衡的。

例子：

 ![](/images/2017-10-27-Statistics03-KNN-3.jpg)

### 搜索 kd树

给定一个目标点，搜索其最临近。

1. 找到包含该目标点的叶节点
2. 从该叶节点出发，一次退回父节点
3. 不断查找与目标点最近邻的节点，当确认没有时终止

搜索的空间限制在局部，效率大大提高！

具体过程：

![](images/2017-10-27-Statistics03-KNN-4.jpg)

![](images/2017-10-27-Statistics03-KNN-5.jpg)

###kd搜索实例1

![](images/2017-10-27-Statistics03-KNN-6.jpg)

### kd搜索实例2

1. 二叉树搜索：先从（7,2）点开始进行二叉查找，然后到达（5,4），最后到达（2,3），此时搜索路径中的节点为<(7,2)，(5,4)，(2,3)>，首先以（2,3）作为当前最近邻点，计算其到查询点（2.1,3.1）的距离为0.1414，
2. 回溯查找：在得到（2,3）为查询点的最近点之后，回溯到其父节点（5,4），并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以（2.1,3.1）为圆心，以0.1414为半径画圆，如下图所示。发现该圆并不和超平面y = 4交割，因此不用进入（5,4）节点右子空间中(图中灰色区域)去搜索；
3. 最后，再回溯到（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。

![](/images/2017-10-27-Statistics03-KNN-7.jpg)

### kd搜索实例3

#### 查询点（2，4.5）

一个复杂点了例子如查找点为（2，4.5），具体步骤依次如下：

1. 同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径<(7,2)，(5,4)，(4,7)>，但（4,7）与目标查找点的距离为3.202，而（5,4）与查找点之间的距离为3.041，所以（5,4）为查询点的最近点；
2. 以（2，4.5）为圆心，以3.041为半径作圆，如下图所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找，也就是将（2,3）节点加入搜索路径中得<(7,2)，(5,4)，(2,3)>；于是接着搜索至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5；
3. 回溯查找至（5,4），直到最后回溯到根结点（7,2）的时候，以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如下图所示。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.5。

![](/images/2017-10-27-Statistics03-KNN-8.jpg)

总结

![](/images/2017-10-27-Statistics03-KNN-9.jpg)